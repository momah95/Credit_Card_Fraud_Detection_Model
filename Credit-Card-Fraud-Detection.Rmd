---
title: "Credit Card Fraud Detection Model"
author: "Somto Momah"
date: "11/5/2020"
output: html_document
---

```{r}
#load packages
library(ranger)
library(caret)
library(data.table)
library(readr)
library(dplyr)
library(randomForest)
library(ggplot2)
library(party)
library(MLmetrics)
library(jsonlite)
library(e1071)
```

```{r}
#load data
creditcard_data <- read.csv("creditcard.csv")
View(creditcard_data)

creditcard_data2 <- creditcard_data[,-c(1,30,26,24,27,14,16,23,20,9,29,21)]
View(creditcard_data2)
```

```{r}
#Exploration
dim(creditcard_data)
head(creditcard_data,6)
tail(creditcard_data,6)
table(creditcard_data$Class)
summary(creditcard_data$Amount)
names(creditcard_data)

#make class a factor
creditcard_data$Class <- as.factor(creditcard_data$Class)
```

```{r}
#manipulation
#scale the amount and time variable and convert to vector
creditcard_data$Amount<-as.vector(scale(creditcard_data$Amount))
creditcard_data$Time<-as.vector(scale(creditcard_data$Time))


```

```{r}
#modeling
library(caTools)
set.seed(1234)
#split the dataset
data_sample<-sample.split(creditcard_data, SplitRatio =0.80)
train_data<-subset(creditcard_data,data_sample==TRUE)
test_data<-subset(creditcard_data,data_sample==FALSE)

#cross validation
control <- trainControl(method="repeatedcv", number=5, repeats=3)
```

```{r}
#fitting logistic regression model
logistic_model<- train(form=Class~., data=train_data,method="glm", family="binomial",trControl=control,tuneLength = 5)

#decision tree
dct_model<- train(form=Class~., data=train_data,method="rpart",preProcess = c("center", "scale"),trControl=control, tuneLength = 5)

# #ann 
# library(neuralnet)
# ANN_model =neuralnet (Class~.,train_data,linear.output=FALSE)
# plot(ANN_model)
# 
# #gbm(gradient boosting model)
# library(gbm, quietly=TRUE)
# # Get the time to train the GBM model
# system.time(
#        model_gbm <- gbm(Class ~ .
#                , distribution = "bernoulli"
#                , data = rbind(train_data, test_data)
#                , n.trees = 500
#                , interaction.depth = 3
#                , n.minobsinnode = 100
#                , shrinkage = 0.01
#                , bag.fraction = 0.5
#                , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data))
# )
# )
# # Determine best iteration based on test data
# gbm.iter = gbm.perf(model_gbm, method = "test")
# 
# model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)
# #Plot the gbm model
# plot(model_gbm)

#svm
svm<- train(form=Class~., data=train_data, method="svmLinear", preProcess = c("center", "scale"), trControl=control, tuneLength =5)
svm

importance <- varImp(svm, scale= FALSE)
plot(importance)
```

```{r}
#delete unwanted columns
train_new <- train_data[,-c(1,30,26,24,27,14,16,23,20,9,29,21)]
test_new <-  test_data[,-c(1,30,26,24,27,14,16,23,20,9,29,21)]

View(train_new)
#re-run svm
svm2<- train(form=Class~., data=train_new, method="svmLinear", preProcess = c("center", "scale"), trControl=control, tuneLength =5)
importance <- varImp(svm2, scale= FALSE)
plot(importance)
#save model to hard disk
save(svm2, file='Support_Vector_Machine_for_credit_card_data.RData')
```


## Predicting Logistic
```{r}
predlog<-predict(logistic_model,test_data,type="raw")
confusionMatrix(test_data$Class, predlog)
#f1_score
f1_log<-F1_Score(test_new$Class, predlog)
f1_log
```

## Predicting Decision Tree
```{r}
preddct<-predict(dct_model,test_data,type="raw")
#confusion matrix
confusionMatrix(test_data$Class, preddct)
#f1_score
f1_dct<-F1_Score(test_new$Class, preddct)
f1_dct
```

<!-- ## Predicitng Ann -->
<!-- ```{r} -->
<!-- predANN=compute(ANN_model,test_data) -->
<!-- resultANN=predANN$net.result -->
<!-- resultANN=ifelse(resultANN>0.5,1,0) -->

<!-- #confusion matrix -->
<!-- confusionMatrix(test_data$Class, predANN) -->
<!-- #f1_score -->
<!-- f1_ann<-F1_Score(test_new$Class, predANN) -->
<!-- f1_ann -->
<!-- ``` -->

<!-- ##Prediciting gbm -->
<!-- ```{r} -->
<!-- # Plot and calculate AUC on test data -->
<!-- gbm_test = predict(model_gbm, newdata = test_data, n.trees = gbm.iter) -->
<!-- gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red") -->
<!-- #confusion matrix -->
<!-- confusionMatrix(test_data$Class,gbm_test) -->
<!-- #f1_score -->
<!-- f1_gbm<-F1_Score(test_new$Class, gbm_test) -->
<!-- f1_gbm -->
<!-- ``` -->
## Predicting svm
```{r}
predsvm<-predict(svm,test_data,type="raw")
confusionMatrix(test_data$Class, predsvm)
#f1_score
f1_svm<-F1_Score(test_data$Class, predsvm)
f1_svm
```

```{r}
#predict svm2 (with feature selection)
predsvm2<-predict(svm2,test_new,type="raw")
confusionMatrix(test_new$Class, predsvm2)
#f1_score
f1_svm2<-F1_Score(test_new$Class, predsvm2)
f1_svm2
```

## ROC Curve for Decision Tree

```{r}
response <- predictor <- c()
response <- c(response, test_data$Class)
predictor <- c(predictor, preddct)

roc <- plot.roc(response, predictor,  main="ROC for DT",
               ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="black")
```

## ROC Curve for Logistic Regression
```{r}
response2 <- predictor2 <- c()
response2 <- c(response2, test_data$Class)
predictor2 <- c(predictor2, predlog)

roc2<- plot.roc(response2, predictor2,  main="ROC for LR",
                ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="magenta")
```

<!-- ## ROC Curve for ANN -->
<!-- ```{r} -->
<!-- response3 <- predictor3 <- c() -->
<!-- response3 <- c(response3, test_data$Class) -->
<!-- predictor3 <- c(predictor3, predANN) -->

<!-- roc3<- plot.roc(response3, predictor3,  main="ROC for ANN", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="blue") -->
<!-- ``` -->

<!-- ## ROC Curve for GBM -->
<!-- ```{r} -->
<!-- response4 <- predictor4 <- c() -->
<!-- response4 <- c(response4, test_data$Class) -->
<!-- predictor4 <- c(predictor4, gbm_test ) -->

<!-- roc4<- plot.roc(response4, predictor4,  main="ROC for GBM", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="pink") -->
<!-- ``` -->

## ROC Curve for svm
```{r}
response5 <- predictor5 <- c()
response5 <- c(response5, test_data$Class)
predictor5 <- c(predictor5, predsvm)

roc5<- plot.roc(response5, predictor5,  main="ROC for SVM",
                ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="green")
```

All ROC
```{r}
roc <- plot.roc(response, predictor,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="black") 
par(new=TRUE)

roc2 <- plot.roc(response2, predictor2,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="magenta")
par(new=TRUE)

# roc3<- plot.roc(response3, predictor3,  main="ROC for DT, LR,ANN and GBM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="blue")
# 
# roc4<- plot.roc(response4, predictor4,  main="ROC for DT, LR,ANN and GBM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="pink")

roc5<- plot.roc(response5, predictor5,  main="ROC for DT, LR and SVM",
                ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="green")

legend("bottomright", legend = c("DT", "LR",'SVM'), col = c("black", "magenta", "GREEN"),lwd = 2)
```






<!-- ## ALTERNATIVE METHOD -->
<!-- ```{r} -->
<!-- #logistic regression -->
<!-- Logistic_Model=glm(Class~.,train_data,family=binomial()) -->
<!-- summary(Logistic_Model) -->
<!-- View(Logistic_Model) -->

<!-- summary(train_data) -->
<!-- plot(Logistic_Model) -->

<!-- library(pROC) -->
<!-- lr.predict <- predict(Logistic_Model,test_data, probability = TRUE) -->
<!-- table(lr.predict, test_data$Class) -->

<!-- response2 <- predictor2 <- c() -->
<!-- response2 <- c(response2, test_data$Class) -->
<!-- predictor2 <- c(predictor2, lr.predict) -->

<!-- roc2<- plot.roc(response2, predictor2,  main="ROC for LR", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="magenta") -->

<!-- #decision tree -->
<!-- library(rpart) -->
<!-- library(rpart.plot) -->
<!-- decisionTree_model <- rpart(Class ~ . , train_data, method = 'class') -->

<!-- predicted_val <- predict(decisionTree_model, creditcard_data, type = 'class') -->
<!-- probability <- predict(decisionTree_model, creditcard_data, type = 'prob') -->
<!-- rpart.plot(decisionTree_model) -->

<!-- preddct<-predict(decisionTree_model,test_data,type="class") -->
<!-- table(preddct, test_data$Class) -->

<!-- response <- predictor <- c() -->
<!-- response <- c(response, test_data$Class) -->
<!-- predictor <- c(predictor, preddct) -->

<!-- roc<- plot.roc(response, predictor,  main="ROC for DT", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="blue") -->

<!-- #ANN -->
<!-- library(neuralnet) -->
<!-- ANN_model =neuralnet (Class~.,train_data,linear.output=FALSE) -->
<!-- plot(ANN_model) -->

<!-- #predANN=compute(ANN_model,test_data) -->

<!-- predANN=predict(ANN_model,test_data, type="class") -->

<!-- resultANN=predANN$net.result -->
<!-- resultANN=ifelse(resultANN>0.5,1,0) -->

<!-- table(predANN, test_data$Class) -->
<!-- predANN$net.result -->

<!-- response3 <- predictor3 <- c() -->
<!-- response3 <- c(response3, test_data$Class) -->
<!-- predictor3 <- c(predictor3, predANN ) -->

<!-- roc3<- plot.roc(response3, predictor3,  main="ROC for ANN", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="pink") -->



<!-- #gbm(gradient boosting model) -->
<!-- library(gbm, quietly=TRUE) -->
<!-- # Get the time to train the GBM model -->
<!-- system.time( -->
<!--        model_gbm <- gbm(Class ~ . -->
<!--                , distribution = "bernoulli" -->
<!--                , data = rbind(train_data, test_data) -->
<!--                , n.trees = 500 -->
<!--                , interaction.depth = 3 -->
<!--                , n.minobsinnode = 100 -->
<!--                , shrinkage = 0.01 -->
<!--                , bag.fraction = 0.5 -->
<!--                , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data)) -->
<!-- ) -->
<!-- ) -->
<!-- # Determine best iteration based on test data -->
<!-- gbm.iter = gbm.perf(model_gbm, method = "test") -->

<!-- model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE) -->
<!-- #Plot the gbm model -->
<!-- plot(model_gbm) -->

<!-- predgbm<-predict(model_gbm,test_data) -->
<!-- table(predgbm, test_data$Class) -->

<!-- response4 <- predictor4 <- c() -->
<!-- response4 <- c(response4, test_data$Class) -->
<!-- predictor4 <- c(predictor4, gbm_test ) -->

<!-- roc4<- plot.roc(response4, predictor4,  main="ROC for GBM", -->
<!--                 ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="brown") -->
<!-- ``` -->

```{r}
#all roc (receiver operating chracteristics)
roc <- plot.roc(response, predictor,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="blue") 
par(new=TRUE)

roc2 <- plot.roc(response2, predictor2,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="magenta")
par(new=TRUE)

roc3<- plot.roc(response3, predictor3,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="pink")
par(new=TRUE)

roc4<- plot.roc(response4, predictor4,  main="ROC for DT, LR and SVM",ylab="True Positive Rate",xlab="False Positive Rate", percent=TRUE, col="brown")

legend("bottomright", legend = c("DT", "LR",'SVM'), col = c("blue", "magenta", "pink", "brown"),lwd = 2)
```